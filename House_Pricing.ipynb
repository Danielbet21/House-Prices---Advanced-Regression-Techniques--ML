{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary libraries like pandas, matplotlib, seaborn, sklearn, plotly\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn import metrics\n",
    "from sklearn import pipeline\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn import neural_network\n",
    "from sklearn import model_selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import LeavePOut\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all the necessary configurations for the graphs\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.set_context(\"paper\")\n",
    "sns.set_palette(\"muted\")\n",
    "plt.figure(figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input and folder paths for the data\n",
    "input_folder = \"house-prices-advanced-regression-techniques/\"\n",
    "\n",
    "train_data_path = os.path.join(input_folder,\"train.csv\")\n",
    "test_data_path = os.path.join(input_folder,\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Loading the Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Droping the Id feature from the train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_id = train_data['Id']\n",
    "test_data_id = test_data['Id']\n",
    "train_data.drop('Id', axis=1, inplace=True)\n",
    "test_data.drop('Id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Count the number of feuatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of features: {train_data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: <br> *Analyzing the data - EDA*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Get the data types of the columns in the training dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the data is from type Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Data Cleaning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_missing_values_stat(data):\n",
    "    print(\"Missing values in the dataset:\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Total Rows: \", len(data))\n",
    "    print(\"_________________________________________\")\n",
    "    # Display missing values in each column of the training dataset\n",
    "    missing_values = data.isnull().sum()\n",
    "    missing_percentage = (missing_values / len(train_data)) * 100\n",
    "    missing_data = pd.concat([missing_values, missing_percentage], axis=1, keys=['Missing Values', 'Percentage'])\n",
    "    missing_data.sort_values(by='Missing Values', ascending=False, inplace=True)\n",
    "    print(missing_data.head(20))\n",
    "    \n",
    "    print(\"\\n\\nTotal missing values: \", missing_data['Missing Values'].sum())\n",
    "    print(\"-----------------------------------------\")\n",
    "\n",
    "    \n",
    "    \n",
    "show_missing_values_stat(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we can see that there are missing values in all the features above :\n",
    "* LotFrontage - 259 \n",
    "* Alley - 1369 \n",
    "* MasVnrType - 872 <br>.<br>.<br>.\n",
    "* MiscFeature - 1406\n",
    "\n",
    "Total of *19* features with missing values\n",
    "- 3 of float64\n",
    "- 16 of object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - we can see that a lot of the data is missing hance it's will be very hard to fill the missing part and might give us a false information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_highly_missing_features(data, fetures_to_drop):\n",
    "    data = data.drop(fetures_to_drop, axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def find_features_with_missing_values_threshold(data, threshold):\n",
    "    missing_values = data.isnull().sum()\n",
    "    missing_percentage = (missing_values / len(train_data)) * 100\n",
    "    missing_data = pd.concat([missing_values, missing_percentage], axis=1, keys=['Missing Values', 'Percentage'])\n",
    "    missing_data.sort_values(by='Missing Values', ascending=False, inplace=True)\n",
    "    features_to_drop = missing_data[missing_data['Percentage'] > threshold].index\n",
    "    return features_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the threshold for missing values to remove\n",
    "threshold = 80\n",
    "# for 80 it return # [\"Alley\", \"PoolQC\", \"Fence\", \"MiscFeature\"]\n",
    "drop_features = find_features_with_missing_values_threshold(train_data, threshold) \n",
    "\n",
    "\n",
    "train_data = drop_highly_missing_features(train_data, drop_features)\n",
    "\n",
    "test_data = drop_highly_missing_features(test_data, drop_features)\n",
    "\n",
    "print(\"Remove this features: \", drop_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Check the impact of dropping features that have less than 20% data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_missing_values_stat(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *separate the numerical and categorical columns* ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_num = train_data.select_dtypes(include=[np.number])\n",
    "train_data_cat = train_data.select_dtypes(include=[object])\n",
    "\n",
    "test_data_num = test_data.select_dtypes(include=[np.number])\n",
    "test_data_cat = test_data.select_dtypes(include=[object])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: <br>*Handling Missing Data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *use a heat map on the numerical data to see the correlation between the features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graphs = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# changeeeeeeeee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sweetviz as sw\n",
    "if False:\n",
    "    usedcars_report = sw.analyze(train_data)\n",
    "    usedcars_report.show_notebook(layout='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_corr_mat(df):\n",
    "    corr_matrix = df.corr()\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\")\n",
    "    plt.show()\n",
    "\n",
    "if show_graphs:\n",
    "    show_corr_mat(train_data_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low correaltion features: \n",
    " <br>LowQualFinSF----0.03\n",
    " <br>MiscVal-----------0.02\n",
    " <br>MiscVal-----------0.02\n",
    " <br>BsmtFinType2-----0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_from_data_set(df, cols: list):\n",
    "    df.drop(cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_from_data_set(test_data_num, [\"LowQualFinSF\",\"MiscVal\",\"MiscVal\",\"BsmtFinSF2\"])\n",
    "drop_from_data_set(train_data_num, [\"LowQualFinSF\",\"MiscVal\",\"MiscVal\",\"BsmtFinSF2\"])\n",
    "print(test_data_num.shape)\n",
    "print(train_data_num.shape)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The diffrence between the number of features is beacuse the test_data doesn't have the SalePrice feature in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    show_corr_mat(train_data_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Handling Missing Values for Numerical Features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing numerical values with median\n",
    "def handle_missing_values_numerical(data):\n",
    "    for column in data.select_dtypes(include=[np.number]).columns:\n",
    "        data[column] = data[column].fillna(data[column].mean()) \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Handling Missing Values for Categorical Features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing categorical values with most frequent value\n",
    "def handle_missing_values_categorical(data):\n",
    "    for column in data.select_dtypes(include=[object]).columns:\n",
    "        data[column] = data[column].fillna(data[column].mode()[0])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_num = handle_missing_values_numerical(train_data_num)\n",
    "train_data_cat = handle_missing_values_categorical(train_data_cat)\n",
    "\n",
    "test_data_num = handle_missing_values_numerical(test_data_num)\n",
    "test_data_cat = handle_missing_values_categorical(test_data_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing values in the training dataset after filling:\")\n",
    "print(train_data_num.isnull().sum().sum() + train_data_cat.isnull().sum().sum())\n",
    "\n",
    "\n",
    "print(\"\\nMissing values in the test dataset after filling:\")\n",
    "print(test_data_num.isnull().sum().sum() + test_data_cat.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the datasets to ensure consistent one-hot encoding\n",
    "train_data = pd.concat([train_data_cat, train_data_num], axis=1)\n",
    "test_data = pd.concat([test_data_cat, test_data_num], axis=1)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: <br> *Data Visualizing*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Distribution of SalePrice*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if show_graphs:\n",
    "    plt.figure(figsize=(11, 7))\n",
    "    sns.histplot(train_data['SalePrice'], kde=True, bins=30, color='blue')\n",
    "    plt.title('Distribution of SalePrice')\n",
    "    plt.xlabel('SalePrice')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Histogram for SalePrice*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    fig = px.histogram(train_data, x='SalePrice', title='Distribution of SalePrice')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "if show_graphs:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    stats.probplot(train_data['SalePrice'], dist=\"norm\", plot=plt)\n",
    "    plt.title('Normal Probability Plot of SalePrice')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    train_data_num.hist(bins=50, figsize=(22, 25))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def show_top_correlated_features(correlation_matrix, n):\n",
    "    # Display the heatmap of the correlation matrix with numbers in each cell\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=correlation_matrix.values,\n",
    "        x=correlation_matrix.columns,\n",
    "        y=correlation_matrix.columns,\n",
    "        colorscale='Viridis',\n",
    "        text=correlation_matrix.values.round(2),  # Round values for display\n",
    "        texttemplate=\"%{text}\",\n",
    "        showscale=True))\n",
    "    fig.update_layout(title=f\"Top {n} Correlated Features\", width=1000, height=800)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    # Split the data to numerical and categorical columns\n",
    "    numerical_columns = train_data.select_dtypes(include=[\"int64\", 'float64']).columns\n",
    "    categorical_columns = train_data.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "\n",
    "    # Encode the categorical columns\n",
    "    categorical_columns_encoded = pd.get_dummies(train_data[categorical_columns])\n",
    "\n",
    "    # Combine the numerical and encoded categorical columns\n",
    "    train_data_encoded = pd.concat([train_data[numerical_columns], categorical_columns_encoded], axis=1)\n",
    "    # Create a correlation matrix\n",
    "    correlation_matrix = train_data_encoded.corr().abs()\n",
    "    N = 20\n",
    "    # Get the top N correlated features with the target variable\n",
    "    top_correlated_features = correlation_matrix['SalePrice'].sort_values(ascending=False).head(N).index.tolist()\n",
    "\n",
    "    # Filter the correlation matrix to get the top N correlated features\n",
    "    filtered_correlation_matrix = correlation_matrix.loc[top_correlated_features, top_correlated_features]\n",
    "    show_top_correlated_features(filtered_correlation_matrix, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4:<br>  *Feature Engineering*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Square Footage\n",
    "\n",
    "- We create a new feature TotalSF by summing up the total basement square footage, first floor square footage, second floor square footage, and garage area. This feature represents the total square footage of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_engineering_list = []\n",
    "\n",
    "# create TotalSF feature\n",
    "def create_TotalSF_feature(data, features_engineering_list=None):\n",
    "    data['TotalSqureF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF'] + data['GarageArea']\n",
    "    if features_engineering_list is not None:\n",
    "        features_engineering_list.append('TotalSqureF')\n",
    "    return data\n",
    "\n",
    "# Create the TotalSF feature for the train and test data\n",
    "train_data = create_TotalSF_feature(train_data, features_engineering_list)\n",
    "\n",
    "test_data = create_TotalSF_feature(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Age of the House*\n",
    "\n",
    "- We calculate the age of the house at the time of sale by subtracting the year the house was built from the year it was sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create age_of_house feature\n",
    "def create_age_of_house_feature(data, features_engineering_list=None):\n",
    "    data['AgeOfHouse'] = data['YrSold'] - data['YearBuilt']\n",
    "    if features_engineering_list is not None:\n",
    "        features_engineering_list.append('AgeOfHouse')\n",
    "    return data\n",
    "\n",
    "\n",
    "# Create the AgeOfHouse feature for the train and test data\n",
    "train_data = create_age_of_house_feature(train_data, features_engineering_list)\n",
    "\n",
    "test_data = create_age_of_house_feature(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Total Bathrooms*\n",
    "\n",
    "- We create a new feature TotalBath by summing up the number of full and half bathrooms in the basement and above grade, with half bathrooms counted as 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TotalBath feature\n",
    "def create_TotalBath_feature(data, features_engineering_list=None):\n",
    "    data['TotalBath'] = data['FullBath'] + 0.5 * data['HalfBath'] + data['BsmtFullBath'] + 0.5 * data['BsmtHalfBath']\n",
    "    if features_engineering_list is not None:\n",
    "        features_engineering_list.append('TotalBath')\n",
    "    return data\n",
    "\n",
    "# Create the TotalBath feature for the train and test data\n",
    "train_data = create_TotalBath_feature(train_data, features_engineering_list)\n",
    "\n",
    "test_data = create_TotalBath_feature(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Age of the Renovation*\n",
    "\n",
    "- We calculate the age of the house since its most recent renovation by subtracting the year of the most recent renovation from the year it was sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create age_of_renovation feature\n",
    "def create_age_of_renovation_feature(data, features_engineering_list=None):\n",
    "    data['AgeOfRenovation'] = data['YrSold'] - data['YearRemodAdd']\n",
    "    if features_engineering_list is not None:\n",
    "        features_engineering_list.append('AgeOfRenovation')\n",
    "    return data\n",
    "\n",
    "# Create the AgeOfRenovation feature for the train and test data\n",
    "train_data = create_age_of_renovation_feature(train_data, features_engineering_list)\n",
    "\n",
    "test_data = create_age_of_renovation_feature(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Total Porch Area_\n",
    "\n",
    "- We create a new feature TotalPorchSF by summing up the area of all porch-related features, representing the total porch area of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TotalPorchSF feature\n",
    "def create_TotalPorchSF_feature(data, features_engineering_list=None):\n",
    "    data['TotalPorchSF'] = data['OpenPorchSF'] + data['EnclosedPorch'] + data['3SsnPorch'] + data['ScreenPorch']\n",
    "    if features_engineering_list is not None:\n",
    "        features_engineering_list.append('TotalPorchSF')\n",
    "    return data\n",
    "\n",
    "# Create the TotalPorchSF feature for the train and test data\n",
    "train_data = create_TotalPorchSF_feature(train_data, features_engineering_list)\n",
    "\n",
    "test_data = create_TotalPorchSF_feature(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Display the New Features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[features_engineering_list].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    N = 20\n",
    "    # Split the data to numerical and categorical columns\n",
    "    numerical_columns = train_data.select_dtypes(include=[\"int64\", 'float64']).columns\n",
    "    categorical_columns = train_data.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "\n",
    "    # Encode the categorical columns\n",
    "    categorical_columns_encoded = pd.get_dummies(train_data[categorical_columns])\n",
    "\n",
    "    # Combine the numerical and encoded categorical columns\n",
    "    train_data_encoded = pd.concat([train_data[numerical_columns], categorical_columns_encoded], axis=1)\n",
    "\n",
    "\n",
    "    # Create a correlation matrix\n",
    "    correlation_matrix = train_data_encoded.corr().abs()\n",
    "    \n",
    "    # Get the top N correlated features with the target variable\n",
    "    top_correlated_features = correlation_matrix['SalePrice'].sort_values(ascending=False).head(N).index.tolist()\n",
    "\n",
    "    # Filter the correlation matrix to get the top N correlated features\n",
    "    filtered_correlation_matrix = correlation_matrix.loc[top_correlated_features, top_correlated_features]\n",
    "    \n",
    "    show_top_correlated_features(filtered_correlation_matrix, N)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- concat the train and the test.\n",
    "### *Make the One-Hot-Encoding on the data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the SalePrice column\n",
    "sale_price = train_data['SalePrice']\n",
    "\n",
    "# Apply one-hot encoding\n",
    "train_data = pd.get_dummies(train_data)\n",
    "test_data = pd.get_dummies(test_data)\n",
    "\n",
    "# Align the datasets to ensure consistent columns\n",
    "train_data, test_data = train_data.align(test_data, join='inner', axis=1)\n",
    "\n",
    "# Add the SalePrice column back to the training dataset\n",
    "train_data['SalePrice'] = sale_price\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5:<br> *Regularization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # choose the best 3 features of this dataset with SGDRegressor\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.feature_selection import RFE\n",
    "\n",
    "\n",
    "# def feature_selec(X, y, n):\n",
    "#     numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "#     categorical_cols = X.select_dtypes(include=['object', 'bool']).columns\n",
    "#     all_cols = categorical_cols.tolist() + numerical_cols.tolist()\n",
    "#     ct_enc_std = ColumnTransformer([\n",
    "#                 (\"encoding\", OrdinalEncoder(), categorical_cols),\n",
    "#                 (\"standard\", StandardScaler(), numerical_cols)])\n",
    "#     X_encoded = pd.DataFrame(ct_enc_std.fit_transform(X, y), columns=all_cols)\n",
    "\n",
    "#     selector = RFE(SGDRegressor(random_state=1), n_features_to_select=n).\\\n",
    "#     fit(X_encoded, y)\n",
    "\n",
    "#     X_encoded.loc[:, selector.support_]\n",
    "\n",
    "#     # print the fetures selection list\n",
    "#     features = X_encoded.loc[:, selector.support_].columns.tolist()\n",
    "#     print(\"features: \", features)\n",
    "\n",
    "#     # keep only the feature selection list\n",
    "#     X = X[features]\n",
    "#     return X , features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *K-Fold*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataToKFold(X, t, k):\n",
    "    cv = KFold(n_splits=k, shuffle=True, random_state=1)\n",
    "    result = []\n",
    "    \n",
    "    for i, (train_ids, val_ids) in enumerate(cv.split(X)):\n",
    "        X_train = X.loc[train_ids]\n",
    "        t_train = t.loc[train_ids]\n",
    "        X_val = X.loc[val_ids]\n",
    "        t_val = t.loc[val_ids]\n",
    "        \n",
    "        result.append({\"X_train\": X_train\n",
    "                      ,\"t_train\" : t_train\n",
    "                      ,\"X_val\": X_val\n",
    "                      ,\"t_val\": t_val\n",
    "                      })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *LOOCV: Leave-One-Out Cross-Validation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeavePOut\n",
    "\n",
    "\n",
    "def splitDataToLPOCV(X, t, p):\n",
    "    cv = LeavePOut(p)\n",
    "    result = []\n",
    "\n",
    "    for train_ids, val_ids in cv.split(X):\n",
    "        X_train = X.iloc[train_ids]\n",
    "        t_train = t.iloc[train_ids]\n",
    "        X_val = X.iloc[val_ids]\n",
    "        t_val = t.iloc[val_ids]\n",
    "        \n",
    "        result.append({\"X_train\": X_train,\n",
    "                       \"t_train\": t_train,\n",
    "                       \"X_val\": X_val,\n",
    "                       \"t_val\": t_val})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Marge-Cv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def margeCV(cv):\n",
    "    X_train = []\n",
    "    t_train = []    \n",
    "    X_val = []\n",
    "    t_val = []\n",
    "    \n",
    "    for i, d in enumerate(cv):\n",
    "            X_train.append(d[\"X_train\"])\n",
    "            t_train.append(d[\"t_train\"])\n",
    "            \n",
    "            X_val.append(d[\"X_val\"])\n",
    "            t_val.append(d[\"t_val\"])\n",
    "            \n",
    "    X_train = pd.concat(X_train) \n",
    "    t_train = pd.concat(t_train) \n",
    "    X_val = pd.concat(X_val) \n",
    "    t_val = pd.concat(t_val) \n",
    "    \n",
    "    \n",
    "    return {\"X_train\": X_train,\n",
    "            \"t_train\": t_train,\n",
    "            \"X_val\": X_val,\n",
    "            \"t_val\": t_val\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Feature Selection*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE, SelectFromModel, SequentialFeatureSelector\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "def feature_selection(X, y, method='rfe', model=SGDRegressor(random_state=1), n_features=3):\n",
    "    \"\"\"\n",
    "    Select the best features using different feature selection methods.\n",
    "\n",
    "    Parameters:\n",
    "    X (pd.DataFrame): Feature dataset\n",
    "    y (pd.Series): Target vector\n",
    "    method (str): Feature selection method ('rfe', 'forward', 'backward', 'hybrid')\n",
    "    model: Machine learning model for feature selection\n",
    "    n_features (int): Number of features to select\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Dataset with selected features\n",
    "    \"\"\"\n",
    "    # Separate numerical and categorical columns\n",
    "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'bool']).columns\n",
    "    all_cols = categorical_cols.tolist() + numerical_cols.tolist()\n",
    "\n",
    "    # Create a column transformer for encoding and scaling\n",
    "    ct_enc_std = ColumnTransformer([\n",
    "        (\"encoding\", OrdinalEncoder(), categorical_cols),\n",
    "        (\"standard\", StandardScaler(), numerical_cols)\n",
    "    ])\n",
    "\n",
    "    # Encode and standardize the features\n",
    "    X_encoded = pd.DataFrame(ct_enc_std.fit_transform(X, y), columns=all_cols)\n",
    "\n",
    "    # Initialize the selector based on the chosen method\n",
    "    if method == 'rfe':\n",
    "        selector = RFE(model, n_features_to_select=n_features)\n",
    "    elif method == 'forward':\n",
    "        selector = SequentialFeatureSelector(model, n_features_to_select=n_features, direction='forward')\n",
    "    elif method == 'backward':\n",
    "        selector = SequentialFeatureSelector(model, n_features_to_select=n_features, direction='backward')\n",
    "    elif method == 'hybrid':\n",
    "        selector = SelectFromModel(model, max_features=n_features)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose from 'rfe', 'forward', 'backward', or 'hybrid'.\")\n",
    "\n",
    "    # Fit the selector and transform the dataset\n",
    "    selector.fit(X_encoded, y)\n",
    "    selected_features = X_encoded.columns[selector.get_support()]\n",
    "\n",
    "    return X_encoded.loc[:, selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Call the feature Selection By Different Usage*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# X and y are your feature matrix and target vector respectively\n",
    " n = 29\n",
    "# For RFE\n",
    "best_features_rfe = feature_selection(X, t, method='rfe', n_features=n)\n",
    "print(\"Best features (RFE):\", best_features_rfe.columns)\n",
    "\n",
    "# For Forward Feature Selection\n",
    "best_features_forward = feature_selection(X, t, method='forward', n_features=n)\n",
    "print(\"Best features (Forward):\", best_features_forward.columns)\n",
    "\n",
    "# For Backward Feature Selection\n",
    "best_features_backward = feature_selection(X, t, method='backward', n_features=n)\n",
    "print(\"Best features (Backward):\", best_features_backward.columns)\n",
    "\n",
    "# For Hybrid Feature Selection\n",
    "best_features_hybrid = feature_selection(X, t, method='hybrid', n_features=n)\n",
    "print(\"Best features (Hybrid):\", best_features_hybrid.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Build The Models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop('SalePrice', axis=1)\n",
    "t = train_data['SalePrice']\n",
    "\n",
    "X_test = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *KFold - making a variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = splitDataToKFold(X, t, 5)\n",
    "cv = margeCV(cv)\n",
    "\n",
    "X_train = cv[\"X_train\"]\n",
    "t_train = cv[\"t_train\"]\n",
    "X_val = cv[\"X_val\"]\n",
    "t_val = cv[\"t_val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "\n",
    "\n",
    "# SGD Regressor\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, random_state=1)\n",
    "sgd_reg.fit(X_train, t_train)\n",
    "sgd_reg_pred = sgd_reg.predict(X_val)\n",
    "sgd_reg_mse = mean_squared_error(t_val, sgd_reg_pred)\n",
    "print(\"SGD Regressor MSE: \", sgd_reg_mse)\n",
    "\n",
    "\n",
    "# Linear Regression\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train, t_train)\n",
    "linear_reg_pred = linear_reg.predict(X_val)\n",
    "linear_reg_mse = mean_squared_error(t_val, linear_reg_pred)\n",
    "print(\"Linear Regression MSE: \", linear_reg_mse)\n",
    "\n",
    "\n",
    "# Ridge Regression\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, t_train)\n",
    "ridge_pred = ridge.predict(X_val)\n",
    "ridge_mse = mean_squared_error(t_val, ridge_pred)\n",
    "print(\"Ridge Regression MSE: \", ridge_mse)\n",
    "\n",
    "\n",
    "# Lasso Regression\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, t_train)\n",
    "lasso_pred = lasso.predict(X_val)\n",
    "lasso_mse = mean_squared_error(t_val, lasso_pred)\n",
    "print(\"Lasso Regression MSE: \", lasso_mse)\n",
    "\n",
    "\n",
    "# Elastic Net Regression\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic_net.fit(X_train, t_train)\n",
    "elastic_net_pred = elastic_net.predict(X_val)\n",
    "elastic_net_mse = mean_squared_error(t_val, elastic_net_pred)\n",
    "print(\"Elastic Net Regression MSE: \", elastic_net_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Choosing The Best Models*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *LPOCV - making a variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Making The Models By KFold*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# split to train and validation\n",
    "cv = splitDataToKFold(X, t, k=5)\n",
    "cv = margeCV(cv)\n",
    "\n",
    "# use lasso model\n",
    "model = Lasso(alpha=0.1)\n",
    "print(\"Done with Lasso\")\n",
    "# feature selection\n",
    "\n",
    "print(\"Done with feature selection\")\n",
    "model.fit(cv[\"X_train\"],cv[\"t_train\"])\n",
    "\n",
    "print(f\"Train Score: {model.score(cv['X_train'], cv['t_train'])}\\n\\\n",
    "Validation Score: {model.score(cv['X_val'], cv['t_val'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Prediction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "test_data[\"Id\"] = test_data_id\n",
    "output = pd.DataFrame({'Id': test_data['Id'], 'SalePrice': predictions})\n",
    "output.to_csv('submission.csv', index=False)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
